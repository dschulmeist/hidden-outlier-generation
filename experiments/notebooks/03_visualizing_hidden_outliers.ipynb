{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Experiment 3: What Do Hidden Outliers Look Like?\n",
    "\n",
    "This experiment visually demonstrates the difference between generating hidden outliers in **latent space** versus **pixel space**.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "A natural question arises: if we generate hidden outliers for image data, what do they actually look like?\n",
    "\n",
    "The answer reveals a key insight about why manifold-based generation is superior:\n",
    "- **Latent space outliers**: Look like slightly distorted but recognizable digits\n",
    "- **Pixel space outliers**: Look like random noise added to images\n",
    "\n",
    "## Why the Difference?\n",
    "\n",
    "When generating in latent space:\n",
    "- BISECT finds outliers in a **meaningful** low-dimensional representation\n",
    "- Decoding maps these back to the **manifold of plausible images**\n",
    "- The decoder acts as a \"denoiser,\" constraining outputs to look like real data\n",
    "\n",
    "When generating directly in pixel space:\n",
    "- BISECT can only examine a tiny fraction of 2^784 possible subspaces\n",
    "- Perturbations spread across unexamined subspaces as random noise\n",
    "- No constraint forces outputs to look like real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "from hog_bisect import BisectHOGen\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "except ImportError:\n",
    "    from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten and normalize\n",
    "x_train_flat = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "x_test_flat = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# Use a subset of digit '3' for this experiment\n",
    "DIGIT = 3\n",
    "N_SAMPLES = 500\n",
    "\n",
    "X_data = x_train_flat[y_train == DIGIT][:N_SAMPLES]\n",
    "print(f\"Using {len(X_data)} samples of digit '{DIGIT}'\")\n",
    "print(f\"Data shape: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some original samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "fig.suptitle(f'Original MNIST Digit {DIGIT} Samples', fontsize=14)\n",
    "\n",
    "for i in range(20):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(X_data[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/original_digits.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latent-header",
   "metadata": {},
   "source": [
    "## 2. Generate Hidden Outliers in Latent Space\n",
    "\n",
    "First, we project to a low-dimensional latent space using PCA, generate hidden outliers there, then project back to pixel space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 12  # Low enough for tractable subspace enumeration\n",
    "\n",
    "pca = PCA(n_components=LATENT_DIM, random_state=42)\n",
    "X_latent = pca.fit_transform(X_data)\n",
    "\n",
    "explained_var = np.sum(pca.explained_variance_ratio_) * 100\n",
    "print(f\"Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"Variance explained: {explained_var:.1f}%\")\n",
    "print(f\"Latent shape: {X_latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-latent-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hidden outliers in latent space\n",
    "gen_latent = BisectHOGen(\n",
    "    data=X_latent,\n",
    "    outlier_detection_method=LOF,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "hidden_latent = gen_latent.fit_generate(\n",
    "    gen_points=100,\n",
    "    get_origin_type='weighted',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(hidden_latent)} hidden outliers in latent space\")\n",
    "gen_latent.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-latent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode back to pixel space using PCA inverse transform\n",
    "hidden_latent_decoded = pca.inverse_transform(hidden_latent)\n",
    "\n",
    "# Clip to valid range [0, 1]\n",
    "hidden_latent_decoded = np.clip(hidden_latent_decoded, 0, 1)\n",
    "\n",
    "print(f\"Decoded hidden outliers shape: {hidden_latent_decoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-latent-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show hidden outliers generated in latent space\n",
    "n_show = min(20, len(hidden_latent_decoded))\n",
    "rows = 2 if n_show > 10 else 1\n",
    "cols = min(10, n_show)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "fig.suptitle('Hidden Outliers Generated in LATENT Space (then decoded)', fontsize=14)\n",
    "\n",
    "if rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_show):\n",
    "    ax = axes[i // cols][i % cols] if rows > 1 else axes[i % cols]\n",
    "    ax.imshow(hidden_latent_decoded[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/hidden_outliers_from_latent.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pixel-header",
   "metadata": {},
   "source": [
    "## 3. Generate Hidden Outliers Directly in Pixel Space\n",
    "\n",
    "Now let's generate hidden outliers directly in the 784-dimensional pixel space. Due to computational constraints, BISECT will only examine a subset of possible subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-pixel-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hidden outliers directly in pixel space\n",
    "# Note: With 784 dimensions, BISECT will use random subspace sampling\n",
    "# max_dimensions=11 means it samples 2^11 = 2048 subspaces (vs 2^784 total)\n",
    "\n",
    "gen_pixel = BisectHOGen(\n",
    "    data=X_data,\n",
    "    outlier_detection_method=LOF,\n",
    "    seed=42,\n",
    "    max_dimensions=11  # Limit subspace enumeration\n",
    ")\n",
    "\n",
    "hidden_pixel = gen_pixel.fit_generate(\n",
    "    gen_points=100,\n",
    "    get_origin_type='weighted',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(hidden_pixel)} hidden outliers in pixel space\")\n",
    "gen_pixel.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-pixel-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show hidden outliers generated directly in pixel space\n",
    "n_show = min(20, len(hidden_pixel))\n",
    "rows = 2 if n_show > 10 else 1\n",
    "cols = min(10, n_show)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "fig.suptitle('Hidden Outliers Generated Directly in PIXEL Space (784 dims)', fontsize=14)\n",
    "\n",
    "if rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(n_show):\n",
    "    ax = axes[i // cols][i % cols] if rows > 1 else axes[i % cols]\n",
    "    img = np.clip(hidden_pixel[i], 0, 1)  # Clip to valid range\n",
    "    ax.imshow(img.reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/hidden_outliers_from_pixel.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "side-by-side",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison figure\n",
    "n_compare = min(10, len(hidden_latent_decoded), len(hidden_pixel))\n",
    "\n",
    "fig, axes = plt.subplots(3, n_compare, figsize=(15, 5))\n",
    "fig.suptitle('Comparison: Original vs Latent-Space Outliers vs Pixel-Space Outliers', fontsize=14)\n",
    "\n",
    "for i in range(n_compare):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_data[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', rotation=0, labelpad=50, fontsize=12)\n",
    "    \n",
    "    # Latent space outliers (decoded)\n",
    "    axes[1, i].imshow(hidden_latent_decoded[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Latent\\nSpace', rotation=0, labelpad=50, fontsize=12)\n",
    "    \n",
    "    # Pixel space outliers\n",
    "    img = np.clip(hidden_pixel[i], 0, 1)\n",
    "    axes[2, i].imshow(img.reshape(28, 28), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('Pixel\\nSpace', rotation=0, labelpad=50, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/hidden_outliers_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 5. Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(images, name):\n",
    "    \"\"\"Compute statistics about generated images.\"\"\"\n",
    "    images = np.clip(images, 0, 1)\n",
    "    \n",
    "    # Mean pixel value\n",
    "    mean_pixel = np.mean(images)\n",
    "    \n",
    "    # Standard deviation of pixel values\n",
    "    std_pixel = np.std(images)\n",
    "    \n",
    "    # Sparsity (fraction of near-zero pixels)\n",
    "    sparsity = np.mean(images < 0.1)\n",
    "    \n",
    "    # Distance from original data (mean L2)\n",
    "    if len(images) > 0 and len(X_data) > 0:\n",
    "        min_dists = []\n",
    "        for img in images[:50]:  # Limit for speed\n",
    "            dists = np.linalg.norm(X_data - img, axis=1)\n",
    "            min_dists.append(np.min(dists))\n",
    "        mean_min_dist = np.mean(min_dists)\n",
    "    else:\n",
    "        mean_min_dist = np.nan\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean pixel value: {mean_pixel:.3f}\")\n",
    "    print(f\"  Std pixel value: {std_pixel:.3f}\")\n",
    "    print(f\"  Sparsity (pixels < 0.1): {sparsity:.1%}\")\n",
    "    print(f\"  Mean min L2 dist to original: {mean_min_dist:.3f}\")\n",
    "\n",
    "compute_stats(X_data, \"Original Data\")\n",
    "compute_stats(hidden_latent_decoded, \"Latent Space Outliers\")\n",
    "compute_stats(hidden_pixel, \"Pixel Space Outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment visually demonstrates a key insight:\n",
    "\n",
    "### Latent Space Generation\n",
    "- Hidden outliers look like **slightly distorted but recognizable digits**\n",
    "- The PCA inverse transform (decoder) acts as a constraint, mapping outliers back onto the manifold of plausible images\n",
    "- These outliers are **semantically meaningful** - they represent variations that push the boundaries of what the model considers \"normal\"\n",
    "\n",
    "### Pixel Space Generation\n",
    "- Hidden outliers look like **noisy, degraded images**\n",
    "- Because BISECT can only examine a tiny fraction of possible subspaces (2^11 out of 2^784), perturbations spread as random noise\n",
    "- The \"outlierness\" is distributed across unexamined dimensions\n",
    "\n",
    "### Implications\n",
    "\n",
    "1. **Manifold learning makes hidden outlier generation tractable** - we work with 12 dimensions instead of 784\n",
    "2. **The decoder constrains outputs to be realistic** - generated outliers inherit the structure of real data\n",
    "3. **Quality vs computational cost** - latent space generation produces more meaningful outliers with less computation\n",
    "\n",
    "This explains why the approach from Experiments 1 and 2 works: the latent space captures meaningful structure, and hidden outliers generated there represent genuine boundary cases rather than random noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
